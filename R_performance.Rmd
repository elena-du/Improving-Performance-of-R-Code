---
title: "R_performance"
author: "Elena Dubova"
date: "10/22/2019"
output: html_document
---

# 1. Time efficiency: recursive function vs sequential function with intermediate results stored in a vector
```{r}
#1. Recursive function
fibonacci_rec <- function(n) { 
  if (n <= 1) { 
  return(n) 
  } 
  return(fibonacci_rec(n - 1) + fibonacci_rec(n - 2))
  }

tic  = Sys.time()
fibonacci_rec(40)
toc  = Sys.time()
(time1 = toc-tic)

#2. Sequence, each number is computed only once.
fibonacci_seq <- function(n) { 
  if (n <= 1) { 
    return(n) 
  } # (n+1)th element of this vector is the nth Fibonacci number 
  fib <- rep.int(NA_real_, n + 1) 
  fib[1] <- 0 
  fib[2] <- 1 
  for (i in 2:n) { 
    fib[i + 1] <- fib[i] + fib[i - 1] 
  } 
  return(fib[n + 1])
  }

tic  = Sys.time()
fibonacci_seq(40)
toc  = Sys.time()
(time1 = toc-tic)
```

# 2. Measuring execution time
## system.time()
```{r system.time}

(t = system.time(runif(1e8)))
t[1]+t[2]==t[3]
t[1]+t[2]

print(unclass(system.time(runif(1e8))))

```

User: execution of user instructions of the given expression.
System: execution of system instructions on behalf of the given expression.
Elpsed: total clock time taken to execute the given expression.

If elapsed time is longer than the sum of user time and system time: CPU is multitasking on other processes or it has to wait for resources such as files and network connections to be available. If elapsed time is shorter than the sum of user time and system time: multiple threads or CPUs are used to execute the expression. 

## rbenchmark package, benchmark() function

```{r}

#install.packages("rbenchmark")
library(rbenchmark)

(bench1 <- benchmark(runif(1e8), replications=10))

```

```{r}

within(bench1, 
       { elapsed.mean <- elapsed/replications 
         user.self.mean <- user.self/replications 
         sys.self.mean <- sys.self/replications })

benchmark(runif(1e8), replications=rep.int(1, 10))

```

Execution times for each repetition.

```{r}

benchmark(runif(1e8), replications=rep.int(1, 10))

```

## rbenchmark()
microbenchmark() function only measures the elapsed time and not the user time or system time. It gives an idea of how the execution times across repeated runs are distributed. Therefore, the function is very handy to measure short running tasks over many repetitions. 

```{r}

library(microbenchmark)
(microbenchmark = microbenchmark(runif(1e8), times = 10))

```

# 3. Profiling execution time.

## Profiling a function: Rprof()

```{r}

#sampvar() function is analogous to var()

# Compute sample variance of numeric vector 
sampvar <- function(x) { 
   # Compute sum of vector x 
  my.sum <- function(x) { 
    sum <- 0 
    for (i in x) { 
      sum <- sum + i 
    } 
    sum 
  } 
  # Compute sum of squared variances of the elements of x from # the mean mu
  sq.var <- function(x, mu) { 
    sum <- 0 
    for (i in x) { 
      sum <- sum + (i - mu) ^ 2 
    } 
    sum 
  } 
  
  mu <- my.sum(x) / length(x) 
  sq <- sq.var(x, mu) 
  
  sq / (length(x) - 1)
  
  }

```

```{r}
x <- runif(1e7)
#Rprof.out is the name of a file in which the profiling data is stored. It will be stored in R's current working directory unless another file path is specified.
Rprof("Rprof.out") 
y <- sampvar(x)
#Rprof(NULL) stops profiling. 
Rprof(NULL)

summaryRprof("Rprof.out")

```
self.time and self.pct columns represent the elapsed time for each function. 
total.time and total.pct columns represent the total elapsed time for each function including the time spent inside function calls


```{r}

summaryRprof("Rprof.out")$by.self

```
```{r}

summaryRprof("Rprof.out")$by.total

```

# **proftools** package 

To use the package, one needs to install the graph and Rgraphviz packages from the Bioconductor repository

```{r}

#install.packages("proftools")
if (!requireNamespace("BiocManager", quietly = TRUE))
    install.packages("BiocManager")

BiocManager::install("graph")

if (!requireNamespace("BiocManager", quietly = TRUE))
    install.packages("BiocManager")

BiocManager::install("Rgraphviz")

library(proftools)

```
```{r}
p <- readProfileData(filename="Rprof.out")
plotProfileCallGraph(p, style=google.style, score="total")

```

# 4. Profiling memory utilization.

```{r}
x <- runif(1e7)

#Rprof.out is the name of a file in which the profiling data is stored. It will be stored in R's current working directory unless another file path is specified.
Rprof("Rprof-mem.out", memory.profiling=TRUE) 
y <- sampvar(x)
#y <- sampvar(runif(1e7)) #non-zero example of memory usage statistics

#Rprof(NULL) stops profiling. 
Rprof(NULL)

#setting the memory.profoling argument 'memory' to profile memory utilization. 
summaryRprof("Rprof-mem.out", memory="both")

```

```{r}

print(object.size(x), units="auto")

```

Grabage collector helps release the memory that is unused to the operating system. 

```{r}

gc(T)
y <- sampvar(x)
gc(F)

```

# 5. Identifying and resolving bottlenecks. 

We start by identifying the largest performance bottlenecks using the techniques above and try to solve them. 

## Converting loops to vectorized operations
First, we optimize sq.var() becsuse it consumes the largest time. 
```{r}

# Compute sample variance of numeric vector 
sampvar1 <- function(x) { 
   # Compute sum of vector x 
  my.sum <- function(x) { 
    sum <- 0 
    for (i in x) { 
      sum <- sum + i 
    } 
    sum 
  } 

  mu <- my.sum(x) / length(x) 
  sq <- my.sum((x-mu^2)) #vectorized operation
  
  sq / (length(x) - 1)
  
  }

```

```{r}

x <- runif(1e7)
Rprof("Rprof-mem.out", memory.profiling=TRUE)
y <- sampvar1(x)
Rprof(NULL)
summaryRprof("Rprof-mem.out", memory="both")

```

Second bottleneck to remove is my.sum() function. 
```{r}

# Compute sample variance of numeric vector 
sampvar2 <- function(x) { 

  mu <- sum(x) / length(x) 
  sq <- sum((x-mu^2)) #vectorized operation
  
  sq / (length(x) - 1)
  
  }

```

```{r}

x <- runif(1e7)
Rprof("Rprof-mem.out", memory.profiling=TRUE)
y <- sampvar2(x)
Rprof(NULL)
summaryRprof("Rprof-mem.out", memory="both")

```

In two steps time is reduced from 0.6 seconds to 0.06 seconds. We can compare this result to the preformance of **var()** function (it was written in C to optimize performance).

```{r}

x <- runif(1e7)
Rprof("Rprof-mem.out", memory.profiling=TRUE)
y <- var(x)
Rprof(NULL)
summaryRprof("Rprof-mem.out", memory="both")

```

It is even better, 0.04 seconds. 

# 6. First-hand strategies of performance optimization in R.

## Vectorization

Vectorization happens when R operators take vectors as arguments for quick processing of multiple values. Unlike many other languages, such as Java, C, C++ that process values by iterating, in R both iteration and vectorization is possible. Iteration is not a big deal in terms of performance for C and such as it is a compiled language. For R the concequences are more serious as it is an interpreted language, so it has to interpret loops every time it executes. 

```{r}

N <- 1E5
data <- sample(1:30, size=N, replace=T)
system.time({ data_sq1 <- numeric(N) 
for(j in 1:N) { 
  data_sq1[j] <- data[j]^2 
  } 
})

system.time(data_sq2 <- data^2)

```

Note: benefits from vectorization come not exclusively from computation gain itself. A lot of under-the-hood operations are also optimised, such as type checking (done once per vector vs done for every number).

## Built-in Functions

A function in a compiled language is always faster than a function writted in R. Therefore, lot of CRAN packages are implemented in compiled languages (C/C++).

```{r}

data <- rnorm(1E4*1000)
dim(data) <- c(1E4,1000)
system.time(data_sum1 <- apply(data, MARGIN = 1, FUN = sum))
system.time(data_sum2 <- rowSums(data)) #precompiled C function, optimized

```

Example of optimized library in R, which was developed in 1970s in Fortran: Basic Linear Algebra Subprograms (BLAS). For more information refer to http://www.netlib.org/blas/.
For Mac OS these commands enable optimization:

  $ cd /Library/Frameworks/R.framework/Resources/lib
  $ ln -sf /System/Library/Frameworks/Accelerate.framework/Frameworks/vecLib.framework/Versions/Current/libBLAS.dylib libRblas.dylib
  
Note: it is important to be mindful whil performing this operation and check for up to date solution. This solution worked for me after previous code broke R.

  $ cd /Library/Frameworks/R.framework/Resources/lib
  $ ls -la
  $ unlink <symbolic link introduced by ln earnier, marked with l and link information ->>
  $ cd
  $ brew install openblas
  $ ln -sf /usr/local/Cellar/openblas/0.2.12/lib/libopenblas.dylib /Library/Frameworks/R.framework/Resources/lib/libRblas.dylib

There are BLAS versions of Windows and Linux available for you to download. If R is compiled with an enabled BLAS, that is, by setting the configuration option to --enable-BLAS-shlib while compiling R from its source, swapping between BLAS versions is done in a similar manner as in Mac OS X: by replacing the default BLAS library file with the new one. In Windows, the default library is located in R_HOME\bin\x64\Rblas.dll; while in Linux, it is in R_HOME/lib/libRblas.so.

```{r}

data <- rnorm(1E7)
dim(data) <- c(1E4, 1E3)
system.time(data_mul <- t(data) %*% data) ## reduction from 7-8 seconds to 0.5 seconds.

```

## Preallocating memory

Unlike more strongly typed programming languages like C/C++ or Java, that generally require a vector (or array) to be declared, R will not throw any compilation error if a vector's memory is not preallocated. However there are computational advantages of preallocation. 

```{r}
#dynamic alllocation, no preallocation
N <- 1E4

data_series1 <- 1
system.time({
  for (j in 2:N) {
    data_series1 = c(data_series1, data_series1[j-1]+sample(-5:5, size = 1))
  }
})

```

```{r}
#preallocation of memory by declaring a numeric vectorof size N
N <- 1E4
data_series1 <- numeric(N) ##preallocation 
data_series1 <- 1
system.time({
  for (j in 2:N) {
    data_series1 = c(data_series1, data_series1[j-1]+sample(-5:5, size = 1))
  }
})

```

In the example above, there is some time gain, but not a lot. A good example that demonstrates memory preallocaion benefits is 'apply' vs loop implementation. **'apply'** functions automatically preallocate memory and take care of other housekeeping activities like deleting loop indices. But most importantly, they are faster. 

```{r}
#Getting the data 
N <- 1E6
data <- sample(1:30, size=N, replace=T)
```

```{r}
#Loop without preallocation
data_rand1 <- list()
system.time(for(i in 1:N) 
  data_rand1[[i]] <- rnorm(data[i]))

```

```{r}
#Loop with preallocation
data_rand3 <- vector("list", N)
system.time(for(i in 1:N) 
  data_rand3[[i]] <- rnorm(data[i]))

```

```{r}
#Lapply: vectorized function with preallocation 
system.time(data_rand2 <- lapply(data, rnorm))

```

```{r}
library(microbenchmark)
microbenchmark(data_rand2 <- lapply(data, rnorm), 
               for(i in 1:N) data_rand3[[i]] <- rnorm(data[i]))

```

## Use of simpler data structures. 

Even though **data.frame** is more conveniet to use in the realm of data analysis by allowing dofferent variable types in one data frame, this convenience comes at a cost of computation. When performing computations, in many cases **data.frame** is first coerced into **matrix** and the computations themselves are performed on a **metrix**. Therefore, whenever possible, it is recommended to use **matrix** directly for performance. 

There are two tricks, however, to use with **data.frame** object to improve performance when converting to matrix does not work for a given dataset.

1. Subsetting (conditioning rows or columns through logical test).

```{r}

data <- rnorm(1E5*1000)
dim(data) <- c(1E5,1000)
data_df <- data.frame(data)
system.time(data_df[data_df$X100>0 & data_df$X200<0,])

```

2. Using **which()** function to wrap the condition.

```{r}

system.time(data_df[which(data_df$X100>0 & data_df$X200<0),])

```

## Hash tables instead of lists for lookups.

Implemetation of lists in R is not optimized for lookup. It incurs **O(N)** time complexity to perform a lookup on a list of N elements. Hash table's lookups incurs **O(1)** complexity. 

```{r}

#list 
data <- rnorm(1E3)
data_ls <- as.list(data)
names(data_ls) <- paste("V", c(1:1E3), sep="")
index_rand <- sample(1:1E3, size=1000, replace=T)
index <- paste("V", index_rand, sep="")
list_comptime <- sapply(index, FUN=function(x){ system.time(data_ls[[x]])[3]})
sum(list_comptime)

```

```{r}

#hash table
library(hash)
data_h <- hash(names(data_ls), data)
hash_comptime <- sapply(index, FUN=function(x){ 
  system.time(data_h[[x]])[3]
  })

sum(hash_comptime)

```

*!Note, hash takes longer on my machine, I need to see why.*

## Choice of faster packages.

CRAN offers multiple packages with he same functionality. Here are some of the examples.

**fastcluster** over hclast
**fastcluster** uses optimized C++ code.

```{r}

data <- rnorm(1E4*100)
dim(data) <- c(1E4,100)

```

```{r}

library(cluster)
dist_data <- dist(data)
system.time(hc_data <- hclust(dist_data))

```

```{r}

library(fastcluster)
system.time(hc_data <- hclust(dist_data))

```

**princomp** over prcomp 

```{r}

data <- rnorm(1E4*100)
dim(data) <- c(1E4,100)

```

```{r}

system.time(prcomp_data <- prcomp(data))

```

```{r}

system.time(princomp_data <- princomp(data))

```

Other packages, beneficial from computational perspective:
- **fastmatch**: This provides a faster version of base R's match function
- **RcppEigen**: This includes a faster version of linear modeling lm
- **data.table**: This offers faster data manipulation operations compared to the standard data.frame operations
- **dplyr**: This offers a set of tools to manipulate data frame-like objects efficiently

# 7. Using compiled code for greater speed.

R provides the **compiler** package to compile R code beforehand and save R a step or two when we execute the code.

## Compilation with **compile** package.

```{r}

# Compute the n-period moving average of x
mov.avg <- function(x, n=20) { 
  total <- numeric(length(x) - n + 1) 
  for (i in 1:n) { 
    total <- total + x[i:(length(x) - n + i)] 
  } 
  total / n
}

```

```{r}

## compiler is a base package
library(compiler)
mov.avg.compiled0 <- cmpfun(mov.avg, options=list(optimize=0))
mov.avg.compiled1 <- cmpfun(mov.avg, options=list(optimize=1))
mov.avg.compiled2 <- cmpfun(mov.avg, options=list(optimize=2))
mov.avg.compiled3 <- cmpfun(mov.avg, options=list(optimize=3))

```

```{r}

library(microbenchmark)
set.seed(1)
x <- runif(100)
(bench <- microbenchmark(mov.avg(x),
                        mov.avg.compiled0(x), 
                        mov.avg.compiled1(x), 
                        mov.avg.compiled2(x), 
                        mov.avg.compiled3(x)))

```

```{r}

library(ggplot2)
autoplot(bench)
```

The compiler package provides different functions to compile different types of R code:
- **cmpfun()** compiles an R function
- **compile()** compiles an R expression
- **cmpfile()** compiles an R expression stored in a file

## JIT (Just-In-Time) Compilation

The idea is to let R automatically compile code that is executed without explicitly calling compilation functions from **compiler** package. JIT compilation should be activated. 

```{r}

library(compiler)
enableJIT(level=3)

```

The level argument tells R how much code to compile before execution. Valid values for level are:
- **0**: It disables JIT.
- **1**: It compiles functions before their first use.
- **2**: In addition, it compiles functions before they are duplicated. This is useful for some packages like lattice that store functions in lists.
- **3**: It compiles loops before they are executed.

JIT can also be activated from terminal before starting R: **R_ENABLE_JIT=<level>**

```{r}

microbenchmark(mov.avg(x))

```

## Using compiled languages in R.

Even if we use pre-compillaion techniques described above, R still needs to evaluate the code in dynamic fashion (e.g. it checks the type of object before applying any operation). 
The code below required development tools to be installed, including the compiler for C/C++.

```{r}

library(inline)
mov.avg.inline <- cfunction( 
  sig=signature(x="numeric", n="integer"), 
  body=" 
  
  /* Coerce arguments to the correct types needed. x needs to be a numeric vector (type REALSXP), and n needs to be an integer vector (type INTSXP). */ 
  SEXP x2 = PROTECT(coerceVector(x, REALSXP)); 
  SEXP n2 = PROTECT(coerceVector(n, INTSXP));
  
  /* Create accessors to the actual data being pointed to by the two SEXP's. */ 
  double *x_p = REAL(x2); 
  int n_val = asInteger(n2); 
  
  // Vector lengths 
  int x_len = length(x2); 
  int res_len = x_len - n_val + 1; 
  
  /* Create and initialize a numeric vector (type REALSXP) of length res_len, using allocVector(). Since memory is allocated, use PROTECT to protect the object from R's garbage collection. */ 
  SEXP res = PROTECT(allocVector(REALSXP, res_len)); 
  double *res_p = REAL(res); 
  for (int i = 0; i < res_len; i++) { 
    res_p[i] = 0; 
  } 
  
// Compute window sum 
  for (int j = 0; j < n_val; j++) { 
    for (int k = 0; k < res_len; k++) { 
      res_p[k] += x_p[j + k]; 
    } 
  } 
  
// Compute moving average 
for (int l = 0; l < res_len; l++) { 
  res_p[l] /= n_val; 
} 

// Unprotect allocated memory and return results
UNPROTECT(3); 
return res; ", 
language="C" )

```

Valid options for the **language** argument are: 'C', 'C++', 'Fortran', 'F95', 'ObjectiveC', and 'ObjectiveC++'.
**PROTECT()** macro function is an important wrapper as it prevents memory optimization algorithms like garbage collectors in R to remove variables that might be needed for C code to work. **REAL()** macro function returns the pointer to the first element of the array (in the example above, the arry is of type double). **asInteger()** convenience function returns an actual value that corresponds to the first value in the array, not a pointer. These two functions represent two ways to access R data. 

MLA (Modern Language Assoc.)
Lim, Aloysius, and William Tjhi. R High Performance Programming. Packt Publishing, 2015.

APA (American Psychological Assoc.)
Lim, A., & Tjhi, W. (2015). R High Performance Programming. Birmingham, UK: Packt Publishing.






















